{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "AxAw80XXAXEd",
        "outputId": "bfbf041a-1fe4-45ec-edec-ea8ce9019b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c04081bd-5943-43e8-8e7c-1c57aed16205\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c04081bd-5943-43e8-8e7c-1c57aed16205\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r3xFCjjeAWRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlWQkbSjVjah"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/heart.csv\", sep=\";\") #Checking the Orginial Data\n",
        "df = df.iloc[:, 0].str.split(\",\", expand=True) #Splitting columns\n",
        "df.columns = [\n",
        "    \"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\n",
        "    \"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"target\"\n",
        "]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum() #Finding missing data\n",
        "#No missing data, do not have to drop any columns"
      ],
      "metadata": {
        "id": "a27q_324WE51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()\n",
        "df.drop_duplicates(inplace=True) #Removing Duplicates"
      ],
      "metadata": {
        "id": "cnMmHQ0SX-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes #They are all objects, converting to int64\n",
        "int_cols = [\"age\",\"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
        "float_cols = [\"oldpeak\"] #Has a decimal\n",
        "df[int_cols] = df[int_cols].astype(int) #Turn datatypes into an int\n",
        "df[float_cols] = df[float_cols].astype(float) #Turn datatype into float\n",
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "g8AHdZMPYBVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "    'age': 'age',\n",
        "    'sex': 'sex',\n",
        "    'cp': 'chest_pain_type',\n",
        "    'trestbps': 'resting_blood_pressure',\n",
        "    'chol': 'serum_cholesterol',\n",
        "    'fbs': 'fasting_blood_sugar',\n",
        "    'restecg': 'resting_ecg',\n",
        "    'thalach': 'max_heart_rate',\n",
        "    'exang': 'exercise_induced_angina',\n",
        "    'oldpeak': 'st_depression',\n",
        "    'slope': 'st_slope',\n",
        "    'ca': 'num_major_vessels',\n",
        "    'thal': 'thalassemia',\n",
        "    'target': 'heart_disease'\n",
        "}, inplace=True) #Renaming columns for easy understanding.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uS6WZS17l8rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**This shows the distribution between people with heart disease and people wihout heart disease.**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OFHfJ5z8msP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target_counts = df['heart_disease'].value_counts()\n",
        "\n",
        "# Bar plot\n",
        "plt.figure()\n",
        "target_counts.plot(kind='bar')\n",
        "plt.title('Heart Disease Distribution')\n",
        "plt.xlabel('Heart Disease (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Pie chart\n",
        "plt.figure()\n",
        "target_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Proportion of Heart Disease Cases')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KBwayYY_mLhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graphs shows that the data set contains a balanced amount of people with and without heart disease. Slightly higher amount of people have heart disease comapred to people without. Slight bias.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rmcrPeIdm8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Histograms with categorical columns with heart disease**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rlcu9aqJnxCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [\n",
        "    'sex', 'chest_pain_type', 'fasting_blood_sugar',\n",
        "    'resting_ecg', 'exercise_induced_angina',\n",
        "    'st_slope', 'thalassemia'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    pd.crosstab(df[col], df['heart_disease']).plot(kind='bar')\n",
        "    plt.title(f'{col} vs Heart Disease')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Heart Disease')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8IllepdenpV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By reading the graphs, we can conclude that\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Strong potential risk factors according to the dataset**                     \n",
        "\n",
        "*   **Sex** (Women have double the amount of people who have heart disease compared to those who do not have it. This is very skeptical and high likely a selection bias.)\n",
        "*   **Chest pain type** (Those who have chest pain have a very high chance to have heart disease)\n",
        "*   **Exercise Induced Angina** (Those who do not have Exercise Induced Angina tend to have heart disease. This is very skeptical and high likely not a trusted data source.)\n",
        "*   **Thalassemia** (Those with no 2 thalassemia(permanent abnormality) have  a drastically high rate of having heart disease.)\n",
        "*   **St Slope** (Those  with no 2 slope, downsloping. Have a very high likely to have heart disease.)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Weak potential risk factors according to the dataset**\n",
        "\n",
        "All have an equal amount of people who have heart disease and those who do not.\n",
        "*   **Fasting blood sugar**\n",
        "*   **Resting ecg**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzsDfMBAoXgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using group-by to explore risk facotrs**"
      ],
      "metadata": {
        "id": "8HfavLI3wc6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart disease grouped by chest pain type\n",
        "cp_summary = df.groupby('chest_pain_type')['heart_disease'].mean()\n",
        "print(cp_summary)\n",
        "# Heart disease grouped by sex\n",
        "sex_summary = df.groupby('sex')['heart_disease'].mean()\n",
        "print(sex_summary)\n"
      ],
      "metadata": {
        "id": "clIsBluOoi9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Giving us the same conclusion as led from the histograms."
      ],
      "metadata": {
        "id": "11LAQKPMwpkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heat Map for Age with its relationship with other variables/columns.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zQdWTzdrx8fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "bins = [29, 39, 49, 59, 69, 79, 89]\n",
        "labels = ['30-39','40-49','50-59','60-69','70-79','80+']\n",
        "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    plt.figure(figsize=(10,5))\n",
        "\n",
        "    # Group by age_group and the categorical variable, calculate mean heart disease\n",
        "    grouped = df.groupby(['age_group', col])['heart_disease'].mean().unstack()\n",
        "\n",
        "    sns.heatmap(grouped, annot=True, fmt=\".2f\", cmap='rocket')\n",
        "    plt.title(f'Heart Disease Rate by Age Group and {col}')\n",
        "    plt.ylabel('Age Group')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V6sbL-p4vnj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variables relationship with Age** (2 High-risk and 1 Low-risk)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Sex** (High risk) : Have high correlation with age when sex = female. Meaning, most female age group have heart disease in this dataset, where else age group 50-59 & 60-69 have relatively risk compared to other age. (0.71, 0.61) respectively. Men tend to have less heart disease as you go up in age.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Chest Pain Type** (High risk) :\n",
        "*   **Age group by chest pain type = 0**, tend to follow the same correlation with most age groups, around 0.20-0.26. However, age 40-49 have double the amount 0.43. It is still relatively low.\n",
        "\n",
        "*   **Age group by chest pain type = 1**, tend to follow the same correlation with most age groups, around 0.95-1.00. Extremely high. However, age group 50-59 have a relatively high correlation, 0.71, and age group 60-69 have a moderately risk with a 0.5.\n",
        "*   **Age group by chest pain type = 2**, tend to follow a decreasing slope as age group increases. Age group 30-39, have a 1.00(Guaranteed). Age group 40-49 & Age group 50-59 both have 0.82(High). Age group 60-69 & Age group 70-79 both have 0.67(Moderately-High). Having high correlation but a decreasing rate as age increases.\n",
        "*   **Age group by chest pain type = 3**, tend to follow a increasing slope as age group increases. Age group 30-39 having 0.5 (moderate), Age group 40-49 & 50-59 having 0.67 (Moderately-High), Age group 60-69 having 0.78 (High). Age group (70-79) does not have any patients in the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6QK_MgTQx4h_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN TEST SPLIT FROM SCKITT-LEARN**\n",
        "*   **TRAIN (70%)** : Process by which a machine learning model learns patterns from labeled data and make adjustment to its parameters to minimize prediction errors.\n",
        "*   **VALIDATION (15%)** : Compares training models to find the best model to prevent overfitting\n",
        "*   **TEST (15%)**: Uses reserved unused portion of the dataset for the final evaluation of the chosen model. Gives unbiased result. Helps to show how overfitting or underfitting the training-validation was.\n",
        "\n",
        "**Split into 70%/15%/15% because dataset is small (320+sample). This ensures enough data for the model to learn patterns, while keeping validation and split large enough to reliably evalute the performance.**\n",
        "\n",
        "\n",
        "In this case, we do not have any learning models, therefore validation is done to keep a reliable split.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0tiOHEBj_eIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/heart.csv\", sep=\";\")\n",
        "\n",
        "if df.shape[1] == 1:\n",
        "    df = df.iloc[:, 0].str.split(\",\", expand=True)\n",
        "\n",
        "df.columns = [\n",
        "    \"age\",\"sex\",\"chest_pain_type\",\"resting_blood_pressure\",\"serum_cholesterol\",\n",
        "    \"fasting_blood_sugar\",\"resting_ecg\",\"max_heart_rate\",\"exercise_induced_angina\",\n",
        "    \"st_depression\",\"st_slope\",\"num_major_vessels\",\"thalassemia\",\"heart_disease\"\n",
        "]\n",
        "\n",
        "df.columns = [col.strip() for col in df.columns]\n",
        "\n",
        "df = df.apply(pd.to_numeric)\n",
        "\n",
        "df.head() ##Re-doing data columns to prevent any mess-ups"
      ],
      "metadata": {
        "id": "-ywMtz1RHexU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Step 1: Making X and Y axis.\n",
        "X = df.drop('heart_disease', axis=1)\n",
        "Y = df['heart_disease']\n",
        "\n",
        "#Step 2: First split: 70% Train, 30% Temporary\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.30, ##Put them into 30% temporary section\n",
        "    stratify=Y, ##Ensures equal amount of non heart_disease patiten and heart_disease patient\n",
        "    random_state=42 #Randomizing to prevent biasness\n",
        ")\n",
        "\n",
        "#Step 3: Second Split: 15% Validation, 15% Test\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(\n",
        "    X_temp, Y_temp,\n",
        "    test_size = 0.50, #Put them into 15% test\n",
        "    stratify = Y_temp,\n",
        "    random_state = 42\n",
        ")\n",
        "# Step 4: Check the sizes\n",
        "print(\"Training set:\", X_train.shape, Y_train.shape)\n",
        "print(\"Validation set:\", X_val.shape, Y_val.shape)\n",
        "print(\"Test set:\", X_test.shape, Y_test.shape)\n",
        "\n",
        "# Step 5: Check class distribution (optional but good for medical data)\n",
        "print(\"\\nTarget distribution in training set:\\n\", Y_train.value_counts(normalize=True))\n",
        "print(\"Target distribution in validation set:\\n\", Y_val.value_counts(normalize=True))\n",
        "print(\"Target distribution in test set:\\n\", Y_test.value_counts(normalize=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "aQ6iQsnCHw51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shows that stratification works, all having around the same split with people with heart_disease and people who do not have heart disease.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EMby62K1MHMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIRST MACHINE LEARNING MODEL\n",
        "**LOGICAL REGRESSION**\n",
        "\n",
        "---\n",
        "\n",
        "*   Used when the target variable is categorical, usually binary(0 = no heart disease, 1 = heart disease)\n",
        "*   Predicts the probability that a data points belongs to a certain class\n",
        "---\n",
        "\n",
        "*   All features are multiplied by weight eg(z = w1​x1 ​+ w2​x2 ​+ … +wn​xn​ +b)\n",
        "*   Then applied with sigmoid function which converts the features X weight into a probability between 0 and 1\n",
        "P(y=1)=1/1+e^−z1​\n",
        "\n",
        "\n",
        "    *   If P≥ 0.5 → predict class 1 (heart disease)\n",
        "    *   If P < 0.5 → predict class 0 (no heart disease)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoB4GcroNPrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #Important to find the results\n",
        "\n",
        "#Step 1: Initialize Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "#Step 2: Train the model\n",
        "lr_model.fit(X_train, Y_train)\n",
        "\n",
        "#Step 3: Validate on the validation set\n",
        "Y_val_pred = lr_model.predict(X_val)\n",
        "\n",
        "#Evalute Performance\n",
        "print(\"Validation Accuracy:\", accuracy_score(Y_val, Y_val_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(Y_val, Y_val_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_val, Y_val_pred))"
      ],
      "metadata": {
        "id": "izbPoPTNU3xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression gave me the result of estimated 76% Validation Accuracy,\n",
        "now to compare it to another machine learning model.**\n",
        "\n",
        "---\n",
        "\n",
        "# K-Nearest Neighbors (KNN)\n",
        "*   Pick a random data in the dataset and check its nearest K neighbors.\n",
        "*   If overall conclusion is either (0 = No heart disease, 1 = Heart disease) of the selected data and its K neighbors, then it will be its respective conclusion.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oWSYWC2OW6ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Finding the best K by testing all k out.\n",
        "k_values = range(1,21)\n",
        "validation_accuracies =[]\n",
        "\n",
        "for k in k_values:\n",
        "\n",
        "    # Initialize KNN with current K\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    #Train the model\n",
        "    knn.fit(X_train, Y_train)\n",
        "\n",
        "    #Validate on validation set\n",
        "    Y_val_pred = knn.predict(X_val)\n",
        "    acc = accuracy_score(Y_val, Y_val_pred)\n",
        "    validation_accuracies.append(acc)\n",
        "\n",
        "# Find the best K\n",
        "best_k = k_values[validation_accuracies.index(max(validation_accuracies))]\n",
        "print(f\"Best K: {best_k} with validation accuracy: {max(validation_accuracies):.4f}\")\n",
        "\n",
        "#Train final KNN model using best K\n",
        "knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_model.fit(X_train, Y_train)\n",
        "\n",
        "#Validate final KNN model using best K\n",
        "Y_val_pred = knn_model.predict(X_val)\n",
        "print(\"Validation Accuracy:\", accuracy_score(Y_val, Y_val_pred))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ounzgjn2ZuSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Nearest Neighbors gave me the result of estimated 73% Validation Accuracy. Therefore, I will use Logistic Regression as it has a higher validation accuracy to prevent overfitting.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "a6Fsu249dTaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "Y_test_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Test Accuracy:\", accuracy_score(Y_test, Y_test_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_test_pred))"
      ],
      "metadata": {
        "id": "J179o7KwdU57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion:** Validation accuracy (76%) & Test Accuracy (74%). Therefore good fitting. Indicating Logistic Regression model works well without overfitting.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F408qYu6dpYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To improve my training  and test scores. I am going to change logistic regression with feature scaling and class balancing.\n",
        "\n",
        "\n",
        "* Scales all features → improves Logistic Regression performance\n",
        "\n",
        "* Class weighting → handles slight class imbalance automatically\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BJcuGmJSeWTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "# Added class_weight='balanced' in case of slight class imbalance\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model on scaled training data\n",
        "lr_model.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Validate on the scaled validation set\n",
        "Y_val_pred = lr_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate validation performance\n",
        "print(\"Validation Accuracy:\", accuracy_score(Y_val, Y_val_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(Y_val, Y_val_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_val, Y_val_pred))\n",
        "\n",
        "# Test set evaluation\n",
        "y_test_pred = lr_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(Y_test, Y_test_pred))\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "print(confusion_matrix(Y_test, Y_test_pred))\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(classification_report(Y_test, Y_test_pred))"
      ],
      "metadata": {
        "id": "ZQRXESDZf7w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This led the an increase accuracy in validation accuracy to 78% while Test\n",
        "accuracy remains at 74%.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "82lyH1zrgW4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search ON KNN\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What Grid Search does:**\n",
        "\n",
        "*   Tries all hyperparameter combinations\n",
        "*   Performs cross-validation\n",
        "*   Trains multiple models\n",
        "*   Compares their validation scores\n",
        "*   Selects the best combination\n",
        "*   Selects the best combination\n",
        "\n",
        "---\n",
        "**What it does for KNN:**\n",
        "\n",
        "**KNN looks at the closest patients and voting.**\n",
        "\n",
        "Therefore, the important hyperparameters is **n-neighbors(k)**. If k is too small, it becomes **too sensitive and overfit easily**. If its too large, it will become **too smooth and underfit**.\n",
        "\n",
        "Therefore, it will look for the best k to provide the best fitting.\n",
        "\n",
        "**It also looks for balancing**\n",
        "\n",
        "* 'uniform' → every neighbor votes equally\n",
        "\n",
        "* 'distance' → closer neighbors vote more\n",
        "\n",
        "Distance weighting can improve performance because closer patients are usually more similar medically.\n",
        "\n",
        "**The features are on different scales:**\n",
        "\n",
        "age → 29–77\n",
        "\n",
        "cholesterol → 100–500\n",
        "\n",
        "oldpeak → 0–6\n",
        "\n",
        "If you don’t scale:\n",
        "Cholesterol dominates the distance calculation with its large numerical.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3LsbELkiJ7Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN GridSearch\n",
        "knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid_knn = {\n",
        "    'knn__n_neighbors': [3,5,7,9,11],\n",
        "    'knn__weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "grid_knn = GridSearchCV(\n",
        "    knn_pipeline,\n",
        "    param_grid_knn,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_knn.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best KNN Parameters:\", grid_knn.best_params_)\n",
        "print(\"Best KNN CV Score:\", grid_knn.best_score_)"
      ],
      "metadata": {
        "id": "UYqVs7LzJZ2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search On Logistic Regression\n",
        "\n",
        "---\n",
        "**What it does for Logistic Regression**\n",
        "\n",
        "Logistic Regression finds a linear decision boundary (Boundary being 0.5 to help indicate if the result is 1 or 0).\n",
        "\n",
        "**Therefore the important hyperparameter being C** (inverse of regularization strength). Regularization basically penalizes large coefficients so the model doesn’t fit noise in the data.\n",
        "\n",
        "**This will improve the model as:**\n",
        "\n",
        "**Reduces overfitting:** By testing multiple regularization strengths (C), it avoids a model that’s too tightly fit to training folds.\n",
        "\n",
        "**Improves generalization:** Cross-validation ensures the hyperparameters work across different parts of your training data.\n",
        "\n",
        "**Optimizes performance:** Instead of guessing, it mathematically finds the C (and other parameters) that produce the best average score.\n"
      ],
      "metadata": {
        "id": "HhxElxbKVTt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression GridSearch\n",
        "log_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid_log = {\n",
        "    'logreg__C': [0.001, 0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "grid_log = GridSearchCV(\n",
        "    log_pipeline,\n",
        "    param_grid_log,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_log.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best Logistic Parameters:\", grid_log.best_params_)\n",
        "print(\"Best Logistic CV Score:\", grid_log.best_score_)"
      ],
      "metadata": {
        "id": "sAyT9ODNJjQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_knn = grid_knn.best_estimator_\n",
        "best_log = grid_log.best_estimator_\n",
        "\n",
        "print(\"KNN Test Accuracy:\", best_knn.score(X_test, Y_test))\n",
        "print(\"Logistic Test Accuracy:\", best_log.score(X_test, Y_test))"
      ],
      "metadata": {
        "id": "buMGqDnTj0td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that overall, KNN becomes the better learning model with the help of GridSearch. This maybe because k is a very valuable hyperparameter for KNN which led to better results."
      ],
      "metadata": {
        "id": "CLatlr5zW32k"
      }
    }
  ]
}